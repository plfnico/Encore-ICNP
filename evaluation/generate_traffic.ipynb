{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Decoder' from 'encore.model' (/mnt/ssd1/encore/open-source/evaluation/../encore/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeneration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath: sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mencore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Decoder, Sequential\n",
      "File \u001b[0;32m/mnt/ssd1/encore/open-source/evaluation/generation_utils.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mencore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Decoder, Sequential\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgen_dists_encore\u001b[39m(decoder: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, loads: np\u001b[38;5;241m.\u001b[39mndarray, device) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m     15\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(loads)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Decoder' from 'encore.model' (/mnt/ssd1/encore/open-source/evaluation/../encore/model.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time \n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from generation_utils import *\n",
    "if '..' not in sys.path: sys.path.insert(0, '..')\n",
    "from encore.model import Decoder, Sequential\n",
    "from utils.initialization import *\n",
    "from utils.distribution_utils import *\n",
    "from utils.eval import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sys.path[0])\n",
    "dataset_dir = './data/raw/'\n",
    "size_dir = './data/size/'\n",
    "interval_dir = './data/interval/'\n",
    "metadata_dir = './data/metadata/'\n",
    "size_cdf = pd.read_csv('./data/cdf/size_cdf.csv')\n",
    "interval_cdf = pd.read_csv('./data/cdf/interval_cdf.csv')\n",
    "n_size = len(size_cdf) - 1\n",
    "n_interval = len(interval_cdf) - 1\n",
    "files = os.listdir(size_dir)\n",
    "block_size = 30\n",
    "\n",
    "file = 'app_182.txt'\n",
    "app = file.strip('.txt')\n",
    "data = get_data(size_dir, interval_dir, file, n_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_files(app_dir):\n",
    "    def extract_number(file_path):\n",
    "        match = re.search(r'/(\\d+)\\.csv$', file_path)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "    files = os.listdir(app_dir)\n",
    "    files = [os.path.join(app_dir, f) for f in files]\n",
    "    files = sorted(files, key=extract_number)\n",
    "    return files\n",
    "\n",
    "\n",
    "def save_trace_data(trace, save_file):\n",
    "    trace.sort_values(by='time', inplace=True)\n",
    "    trace.reset_index(drop=True, inplace=True)\n",
    "    if save_file is not None:\n",
    "        with open(save_file, 'w') as f:\n",
    "            f.write('{}\\n'.format(len(trace)))\n",
    "            for row in trace.itertuples():\n",
    "                f.write('{:d} {:.6f} {:d}\\n'.format(row.pair, row.time, int(row.size)))\n",
    "\n",
    "\n",
    "def get_trace(trace_id, size_index, metadata, interval_index=None):\n",
    "    size_sequence = sample_sequence(size_index, size_cdf['size'].values)\n",
    "    size_sequence = np.array(size_sequence, dtype=int)\n",
    "    if interval_index is not None:\n",
    "        interval_sequence = sample_sequence(interval_index, interval_cdf['interval'].values)\n",
    "        interval_sequence = np.array(interval_sequence, dtype=float)\n",
    "    else:\n",
    "        mean_size = np.mean(size_sequence)\n",
    "        mean_interval = mean_size / metadata['load']\n",
    "        interval_sequence = np.random.exponential(mean_interval, 999)\n",
    "    interval_sequence = interval_sequence.astype(float)\n",
    "    time_sequence = np.cumsum(interval_sequence)\n",
    "    time_sequence = np.concatenate(([0], time_sequence))\n",
    "    time_sequence += metadata['start_time']\n",
    "    flow_num = int(metadata['flow_num'])\n",
    "    pair_trace = pd.DataFrame({'pair': np.zeros(flow_num), 'time': time_sequence[0:flow_num], 'size': size_sequence[0:flow_num]})\n",
    "    pair_trace['pair'] = trace_id\n",
    "    pair_trace = pair_trace[pair_trace['time'] <= metadata['end_time']]\n",
    "    return pair_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_trace(app, save_file=None):\n",
    "    app_dir = os.path.join(dataset_dir, app)\n",
    "    files = get_sorted_files(app_dir)\n",
    "    trace = pd.DataFrame()\n",
    "    for i, f in enumerate(files):\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        df['pair'] = i\n",
    "        trace = pd.concat([trace, df])\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_common_practice(app, save_file=None):\n",
    "    file = app + '.txt'\n",
    "    data = get_data(size_dir, interval_dir, file, n_interval)\n",
    "    all_data = np.concatenate(data)\n",
    "    all_sizes = all_data // n_interval\n",
    "    size_dist = compute_probability_distribution(all_sizes, n_size)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    trace = pd.DataFrame()\n",
    "    for i, item in metadata.iterrows():\n",
    "        np.random.seed(i)\n",
    "        size_index = np.random.choice(n_size, 1000, p=size_dist)\n",
    "        pair_trace = get_trace(i, size_index, item)\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(app, save_file=None):\n",
    "    file = app + '.txt'\n",
    "    data = get_data(size_dir, interval_dir, file, n_interval)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    trace = pd.DataFrame()\n",
    "    for i, item in metadata.iterrows():\n",
    "        np.random.seed(i)\n",
    "        seq = data[i]\n",
    "        seq = np.append(seq[:-1], seq[0:block_size - 1])\n",
    "        size_index = seq // n_interval\n",
    "        interval_index = seq % n_interval\n",
    "        interval_index = np.random.permutation(interval_index)\n",
    "        pair_trace = get_trace(i, size_index, item, interval_index)\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_size_sample(app, save_file=None):\n",
    "    file = app + '.txt'\n",
    "    data = get_data(size_dir, interval_dir, file, n_interval)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    app_dir = os.path.join(dataset_dir, app)\n",
    "    files = get_sorted_files(app_dir)\n",
    "    trace = pd.DataFrame()\n",
    "    for i, item in metadata.iterrows():\n",
    "        size_index = data[i] // n_interval\n",
    "        size_sequence = sample_sequence(size_index, size_cdf['size'].values)\n",
    "        size_sequence = np.array(size_sequence, dtype=int)\n",
    "        real_data = pd.read_csv(files[i], sep=',')\n",
    "        time_sequence = real_data['time'].values\n",
    "        flow_num = int(item['flow_num'])\n",
    "        size_sequence = size_sequence[0:flow_num]\n",
    "        time_sequence = time_sequence[0:flow_num]\n",
    "        pair_trace = pd.DataFrame({'pair': np.zeros(flow_num), 'time': time_sequence, 'size': size_sequence})\n",
    "        pair_trace['pair'] = i\n",
    "        pair_trace = pair_trace[pair_trace['time'] <= item['end_time']]\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lomas(app, save_file=None):\n",
    "    dists_lomas, lomas_word_prob = gen_dists_lomas('checkpoints/lomas', n_size, n_interval, app)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    trace = pd.DataFrame()\n",
    "    for i, item in metadata.iterrows():\n",
    "        lomas_word_prob[i] = lomas_word_prob[i] / np.sum(lomas_word_prob[i])\n",
    "        seq = generate_sequence_lomas(lomas_word_prob[i], 1000, i)\n",
    "        size_index = seq // n_interval\n",
    "        interval_index = seq % n_interval\n",
    "        pair_trace = get_trace(i, size_index, item, interval_index)\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cvae(app, model_path, save_file=None):\n",
    "    decoder = load_cvae(model_path, n_size, n_interval, device)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    loads = get_loads(metadata_dir + app + '.csv', normalize=True)\n",
    "    num_sample = len(metadata)\n",
    "    size_recon, interval_recon = gen_dists_encore(decoder, loads, device)\n",
    "    trace = pd.DataFrame()\n",
    "    seq_len = 1000\n",
    "    for i, item in metadata.iterrows():\n",
    "        np.random.seed(i)\n",
    "        size_index = np.random.choice(n_size, seq_len, p=size_recon[i])\n",
    "        interval_index = np.random.choice(n_interval, seq_len, p=interval_recon[i])\n",
    "        pair_trace = get_trace(i, size_index, item, interval_index)\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gru(app, model_path, save_file=None):\n",
    "    model = load_gru(model_path, device)\n",
    "    file = app + '.txt'\n",
    "    data = get_data(size_dir, interval_dir, file, n_interval)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    trace = pd.DataFrame()\n",
    "    seq_len = 1000\n",
    "    initial_seed = 0\n",
    "    for i, item in tqdm(metadata.iterrows()):\n",
    "        np.random.seed(i)\n",
    "        seq = data[i]\n",
    "        seq = np.append(seq[:-1], seq[0:block_size-1])\n",
    "        size_dist, interval_dist = get_probability_distributions_from_sequence(seq, n_size, n_interval)\n",
    "        sequence_gen = generate_sequence_encore(model, size_dist, interval_dist, block_size, device, seq_len, initial_seed)\n",
    "        size_index = np.array(sequence_gen) // n_interval\n",
    "        interval_index = np.array(sequence_gen) % n_interval\n",
    "        pair_trace = get_trace(i, size_index, item, interval_index)\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encore(app, gru_path, cvae_path, save_file=None):\n",
    "    sequential = load_gru(gru_path, device)\n",
    "    decoder = load_cvae(cvae_path, n_size, n_interval, device)\n",
    "    metadata = pd.read_csv(metadata_dir + app + '.csv')\n",
    "    loads = get_loads(metadata_dir + app + '.csv', normalize=True)\n",
    "    size_recon, interval_recon = gen_dists_encore(decoder, loads, device)\n",
    "    trace = pd.DataFrame()\n",
    "    seq_len = 1000\n",
    "    initial_seed = 0\n",
    "    for i, item in tqdm(metadata.iterrows()):\n",
    "        np.random.seed(i)\n",
    "        sequence_gen = generate_sequence_encore(sequential, size_recon[i], interval_recon[i], block_size, device, seq_len, initial_seed)\n",
    "        size_index = np.array(sequence_gen) // n_interval\n",
    "        interval_index = np.array(sequence_gen) % n_interval\n",
    "        pair_trace = get_trace(i, size_index, item, interval_index)\n",
    "        trace = pd.concat([trace, pair_trace], ignore_index=True)\n",
    "    save_trace_data(trace, save_file)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [01:47,  1.07it/s]\n",
      "115it [01:48,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "app = 'app_182'\n",
    "model_date = '2024-5-14-20'\n",
    "gru_path = 'gru-{date}/{app}'.format(app=app, date=model_date)\n",
    "cvae_path = 'cvae-{date}/{app}'.format(app=app, date=model_date)\n",
    "real_trace_file = './simulation/data/trace/real_' + app + '.txt'\n",
    "common_practice_file = './simulation/data/trace/common_' + app + '.txt'\n",
    "sample_file = './simulation/data/trace/sample_' + app + '.txt'\n",
    "size_sample_file = './simulation/data/trace/size_sample_' + app + '.txt'\n",
    "gru_file = './simulation/data/trace/gru_' + app + '.txt'\n",
    "cvae_file = './simulation/data/trace/cvae_' + app + '.txt'\n",
    "encore_file = './simulation/data/trace/encore_' + app + '.txt'\n",
    "lomas_file = './simulation/data/trace/lomas_' + app + '.txt'\n",
    "\n",
    "real_trace = generate_real_trace(app, real_trace_file)\n",
    "common_practice = generate_common_practice(app, common_practice_file)\n",
    "sample = generate_sample(app, sample_file)\n",
    "size_sample = generate_size_sample(app, size_sample_file)\n",
    "lomas = generate_lomas(app, lomas_file)\n",
    "gru = generate_gru(app, gru_path, gru_file)\n",
    "cvae = generate_cvae(app, cvae_path, cvae_file)\n",
    "encore = generate_encore(app, gru_path, cvae_path, encore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
