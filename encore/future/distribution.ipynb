{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from typing import List, Tuple\n",
    "from torch.nn import functional as F\n",
    "if '..' not in os.sys.path:os.sys.path.append('..')\n",
    "from utils.distribution_utils import get_probability_distributions_from_sequence\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_size: 30, n_interval: 30, start_token: 900\n"
     ]
    }
   ],
   "source": [
    "dataset = 'icbc'\n",
    "data_dir = f'/mnt/ssd1/hsj/encore/{dataset}'\n",
    "size_cdf_file = os.path.join(data_dir, f'{dataset}_cdf', f'{dataset}_size_cdf_coarse.csv')\n",
    "interval_cdf_file = os.path.join(data_dir, f'{dataset}_cdf', f'{dataset}_interval_cdf_coarse.csv')\n",
    "size_cdf = pd.read_csv(size_cdf_file)\n",
    "interval_cdf = pd.read_csv(interval_cdf_file)\n",
    "n_size, n_interval = len(size_cdf), len(interval_cdf)\n",
    "start_token = n_size * n_interval\n",
    "print(f'n_size: {n_size}, n_interval: {n_interval}, start_token: {start_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7569/7569 [00:00<00:00, 35200.72it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_trainset(pair_index_file:str, n_size:int, n_interval:int, device:torch.device) -> Tuple[torch.utils.data.TensorDataset, np.ndarray, np.ndarray]:\n",
    "    pair_index = []\n",
    "    with open(pair_index_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pair_index.append([int(x) for x in line.strip().split(',')])\n",
    "\n",
    "    size_probs, interval_probs = [], []\n",
    "    for seq in tqdm(pair_index):\n",
    "        seq = np.array(seq[:-1])\n",
    "        size_prob, interval_prob = get_probability_distributions_from_sequence(seq, n_size, n_interval)\n",
    "        size_probs.append(size_prob)\n",
    "        interval_probs.append(interval_prob)\n",
    "\n",
    "    size_probs, interval_probs = np.array(size_probs), np.array(interval_probs)\n",
    "    size_probe_tensor = torch.tensor(size_probs, dtype=torch.float32).to(device)\n",
    "    interval_probe_tensor = torch.tensor(interval_probs, dtype=torch.float32).to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(size_probe_tensor, interval_probe_tensor)\n",
    "    return dataset, size_probs, interval_probs\n",
    "\n",
    "pair_index_file = os.path.join(data_dir, f'{dataset}_pair_index.txt')\n",
    "trainset, size_probs, interval_probs = get_trainset(pair_index_file, n_size, n_interval, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_size, n_interval, hidden_dims, n_latent):\n",
    "        super(VAE, self).__init__()\n",
    "        # initialize the parameters\n",
    "        self.n_size = n_size\n",
    "        self.n_interval = n_interval\n",
    "        self.n_latent = n_latent\n",
    "        self.encorer_hidden_dims = hidden_dims\n",
    "        self.decoder_hidden_dims = hidden_dims\n",
    "        self.decoder_hidden_dims.reverse()\n",
    "        self.fc_mu = nn.Linear(self.encorer_hidden_dims[-1], n_latent)\n",
    "        self.fc_var = nn.Linear(self.encorer_hidden_dims[-1], n_latent)\n",
    "        self.fc_size = nn.Linear(self.decoder_hidden_dims[-1], n_size)\n",
    "        self.fc_interval = nn.Linear(self.decoder_hidden_dims[-1], n_interval)\n",
    "\n",
    "        # construct encoder\n",
    "        encoder_layers = [nn.Linear(n_size + n_interval, hidden_dims[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            encoder_layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # construct decoder\n",
    "        decoder_layers = [nn.Linear(n_latent, hidden_dims[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            decoder_layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x:Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "    \n",
    "    def reparameterize(self, mu:Tensor, log_var:Tensor) -> Tensor:\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z:Tensor) -> Tensor:\n",
    "        h = self.decoder(z)\n",
    "        return self.fc_size(h), self.fc_interval(h)\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        size_logits, interval_logits = self.decode(z)\n",
    "        size_recon = F.softmax(size_logits, dim=-1)\n",
    "        interval_recon = F.softmax(interval_logits, dim=-1)\n",
    "        return size_recon, interval_recon, mu, log_var\n",
    "    \n",
    "    def generate(self, n:int) -> Tuple[Tensor, Tensor]:\n",
    "        z = torch.randn(n, self.n_latent).to(device)\n",
    "        size_logits, interval_logits = self.decode(z)\n",
    "        size_recon = F.softmax(size_logits, dim=-1)\n",
    "        interval_recon = F.softmax(interval_logits, dim=-1)\n",
    "        return size_recon, interval_recon\n",
    "    \n",
    "\n",
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, kld_weight:float=1.0):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        self.kld_weight = kld_weight\n",
    "\n",
    "    def forward(self, size_recon:Tensor, size_target:Tensor, interval_recon:Tensor, interval_target:Tensor, mu:Tensor, log_var:Tensor) -> Tensor:\n",
    "        recon_loss = F.l1_loss(size_recon, size_target) + F.l1_loss(interval_recon, interval_target)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1), dim=0)\n",
    "        weighted_loss = recon_loss + self.kld_weight * kld_loss\n",
    "        return weighted_loss, recon_loss, kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model:nn.Module, dataloader:DataLoader, optimizer:torch.optim.Optimizer, criterion:nn.Module, device:torch.device) -> Tuple[float, float, float]:\n",
    "    model.train()\n",
    "    running_loss, running_recon_loss, running_kld_loss = 0.0, 0.0, 0.0\n",
    "    for size_batch, interval_batch in dataloader:\n",
    "        size_batch, interval_batch = size_batch.to(device), interval_batch.to(device)\n",
    "        input_tensor = torch.cat([size_batch, interval_batch], dim=1)\n",
    "        size_recon, interval_recon, mu, log_var = model(input_tensor)\n",
    "        loss, recon_loss, kld_loss = criterion(size_recon, size_batch, interval_recon, interval_batch, mu, log_var)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * size_batch.size(0)\n",
    "        running_recon_loss += recon_loss.item() * size_batch.size(0)\n",
    "        running_kld_loss += kld_loss.item() * size_batch.size(0)\n",
    "    average_loss = running_loss / len(dataloader.dataset)\n",
    "    average_recon_loss = running_recon_loss / len(dataloader.dataset)\n",
    "    average_kld_loss = running_kld_loss / len(dataloader.dataset)\n",
    "    return average_loss, average_recon_loss, average_kld_loss\n",
    "\n",
    "\n",
    "def train(model:nn.Module, dataloader:DataLoader, optimizer:torch.optim.Optimizer, scheduler:torch.optim.lr_scheduler, n_epochs:int, plot_every:int, init_kld_weight:float, kld_update_every:int, device:torch.device) -> None:\n",
    "    start_time = time.time()\n",
    "    kld_weight = init_kld_weight\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        criterion = WeightedLoss(kld_weight)\n",
    "        train_loss, train_recon_loss, train_kld_loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "        scheduler.step()\n",
    "        if epoch % plot_every == 0:\n",
    "            print(f'Epoch: {epoch}/{n_epochs}, Epoch time: {time.time() - epoch_start_time:.2f}s, Total time: {time.time() - start_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.2e}, KLD Weight: {kld_weight:.2e}')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Recon Loss: {train_recon_loss:.4f}, Train KLD Loss: {train_kld_loss:.4f}')\n",
    "            print('-' * 80)\n",
    "        if epoch % kld_update_every == 0:\n",
    "            kld_weight = min(kld_weight * 10, 1e-3)\n",
    "    print(f'Training finished after {n_epochs} epochs in {time.time() - start_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]          15,616\n",
      "              ReLU-2                  [-1, 256]               0\n",
      "            Linear-3                  [-1, 512]         131,584\n",
      "              ReLU-4                  [-1, 512]               0\n",
      "            Linear-5                 [-1, 1024]         525,312\n",
      "              ReLU-6                 [-1, 1024]               0\n",
      "            Linear-7                 [-1, 1024]       1,049,600\n",
      "              ReLU-8                 [-1, 1024]               0\n",
      "            Linear-9                  [-1, 512]         524,800\n",
      "             ReLU-10                  [-1, 512]               0\n",
      "           Linear-11                  [-1, 256]         131,328\n",
      "             ReLU-12                  [-1, 256]               0\n",
      "           Linear-13                   [-1, 64]          16,448\n",
      "           Linear-14                   [-1, 64]          16,448\n",
      "           Linear-15                  [-1, 256]          16,640\n",
      "             ReLU-16                  [-1, 256]               0\n",
      "           Linear-17                  [-1, 512]         131,584\n",
      "             ReLU-18                  [-1, 512]               0\n",
      "           Linear-19                 [-1, 1024]         525,312\n",
      "             ReLU-20                 [-1, 1024]               0\n",
      "           Linear-21                 [-1, 1024]       1,049,600\n",
      "             ReLU-22                 [-1, 1024]               0\n",
      "           Linear-23                  [-1, 512]         524,800\n",
      "             ReLU-24                  [-1, 512]               0\n",
      "           Linear-25                  [-1, 256]         131,328\n",
      "             ReLU-26                  [-1, 256]               0\n",
      "           Linear-27                   [-1, 30]           7,710\n",
      "           Linear-28                   [-1, 30]           7,710\n",
      "================================================================\n",
      "Total params: 4,805,820\n",
      "Trainable params: 4,805,820\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 18.33\n",
      "Estimated Total Size (MB): 18.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "hidden_dims = [256, 512, 1024, 1024, 512, 256]\n",
    "latent_dim = 64\n",
    "vae = VAE(n_size, n_interval, hidden_dims, latent_dim)\n",
    "vae = vae.to('cpu')\n",
    "summary(vae, input_size=(n_size + n_interval,), device='cpu')\n",
    "vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /mnt/ssd1/hsj/encore/icbc/model/vae-01-08/encore_vae_70000.pt\n"
     ]
    }
   ],
   "source": [
    "model_dir = f'/mnt/ssd1/hsj/encore/{dataset}/model/vae-01-08/'\n",
    "model_path = os.path.join(model_dir, 'encore_vae_70000.pt')\n",
    "if os.path.exists(model_path):\n",
    "    vae.load_state_dict(torch.load(model_path))\n",
    "    print(f'Loaded model from {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 4805820\n"
     ]
    }
   ],
   "source": [
    "params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print(f'Total trainable parameters: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "size_batch, interval_batch = next(iter(dataloader))\n",
    "input_tensor = torch.cat([size_batch, interval_batch], dim=1)\n",
    "size_recon, interval_recon, mu, log_var = vae(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/10000, Epoch time: 0.21s, Total time: 23.65s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0067, Train KLD Loss: 8.0213\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 200/10000, Epoch time: 0.23s, Total time: 46.50s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0154, Train Recon Loss: 0.0070, Train KLD Loss: 8.3058\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 300/10000, Epoch time: 0.23s, Total time: 69.32s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0070, Train KLD Loss: 7.8934\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 400/10000, Epoch time: 0.09s, Total time: 85.33s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0150, Train Recon Loss: 0.0070, Train KLD Loss: 7.9672\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 500/10000, Epoch time: 0.09s, Total time: 95.14s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0150, Train Recon Loss: 0.0068, Train KLD Loss: 8.1392\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 600/10000, Epoch time: 0.09s, Total time: 105.83s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0070, Train KLD Loss: 7.8881\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 700/10000, Epoch time: 0.21s, Total time: 125.05s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0152, Train Recon Loss: 0.0073, Train KLD Loss: 7.9073\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 800/10000, Epoch time: 0.22s, Total time: 147.92s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0070, Train KLD Loss: 7.7128\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 900/10000, Epoch time: 0.28s, Total time: 172.85s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0070, Train KLD Loss: 7.7224\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1000/10000, Epoch time: 0.23s, Total time: 194.82s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0068, Train KLD Loss: 7.7621\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1100/10000, Epoch time: 0.23s, Total time: 217.60s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0070, Train KLD Loss: 7.7921\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1200/10000, Epoch time: 0.23s, Total time: 240.51s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0070, Train KLD Loss: 7.8626\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1300/10000, Epoch time: 0.23s, Total time: 263.18s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0071, Train KLD Loss: 7.8439\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1400/10000, Epoch time: 0.23s, Total time: 286.08s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0072, Train KLD Loss: 7.6960\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1500/10000, Epoch time: 0.31s, Total time: 309.24s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0152, Train Recon Loss: 0.0073, Train KLD Loss: 7.8506\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1600/10000, Epoch time: 0.23s, Total time: 332.02s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0069, Train KLD Loss: 7.8208\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1700/10000, Epoch time: 0.23s, Total time: 354.95s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0071, Train KLD Loss: 7.7797\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1800/10000, Epoch time: 0.23s, Total time: 377.87s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0071, Train KLD Loss: 7.7190\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1900/10000, Epoch time: 0.23s, Total time: 400.85s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0152, Train Recon Loss: 0.0073, Train KLD Loss: 7.8622\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2000/10000, Epoch time: 0.23s, Total time: 423.88s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0069, Train KLD Loss: 7.6869\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2100/10000, Epoch time: 0.23s, Total time: 446.42s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0150, Train Recon Loss: 0.0073, Train KLD Loss: 7.7345\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2200/10000, Epoch time: 0.19s, Total time: 469.27s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0147, Train Recon Loss: 0.0070, Train KLD Loss: 7.7307\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2300/10000, Epoch time: 0.23s, Total time: 492.03s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0151, Train Recon Loss: 0.0074, Train KLD Loss: 7.7521\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2400/10000, Epoch time: 0.23s, Total time: 517.28s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0150, Train Recon Loss: 0.0072, Train KLD Loss: 7.7618\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2500/10000, Epoch time: 0.19s, Total time: 544.37s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0068, Train KLD Loss: 7.7649\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2600/10000, Epoch time: 0.23s, Total time: 558.84s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0150, Train Recon Loss: 0.0072, Train KLD Loss: 7.8347\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2700/10000, Epoch time: 0.09s, Total time: 573.19s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0068, Train KLD Loss: 7.7151\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2800/10000, Epoch time: 0.28s, Total time: 592.72s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0068, Train KLD Loss: 7.7434\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2900/10000, Epoch time: 0.23s, Total time: 617.41s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0068, Train KLD Loss: 7.7503\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3000/10000, Epoch time: 0.23s, Total time: 640.97s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0071, Train KLD Loss: 7.6839\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3100/10000, Epoch time: 0.23s, Total time: 664.78s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0145, Train Recon Loss: 0.0069, Train KLD Loss: 7.6343\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3200/10000, Epoch time: 0.23s, Total time: 687.58s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0071, Train KLD Loss: 7.6820\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3300/10000, Epoch time: 0.24s, Total time: 711.22s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0147, Train Recon Loss: 0.0070, Train KLD Loss: 7.7254\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3400/10000, Epoch time: 0.23s, Total time: 734.34s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0145, Train Recon Loss: 0.0068, Train KLD Loss: 7.6689\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3500/10000, Epoch time: 0.15s, Total time: 757.80s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0145, Train Recon Loss: 0.0068, Train KLD Loss: 7.7320\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3600/10000, Epoch time: 0.21s, Total time: 772.75s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0147, Train Recon Loss: 0.0070, Train KLD Loss: 7.7347\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3700/10000, Epoch time: 0.23s, Total time: 795.64s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0068, Train KLD Loss: 8.1406\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3800/10000, Epoch time: 0.23s, Total time: 818.58s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0147, Train Recon Loss: 0.0070, Train KLD Loss: 7.7420\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 3900/10000, Epoch time: 0.24s, Total time: 841.63s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0071, Train KLD Loss: 7.7734\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4000/10000, Epoch time: 0.21s, Total time: 864.70s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0157, Train Recon Loss: 0.0079, Train KLD Loss: 7.8279\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4100/10000, Epoch time: 0.23s, Total time: 887.04s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0148, Train Recon Loss: 0.0070, Train KLD Loss: 7.7862\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4200/10000, Epoch time: 0.10s, Total time: 904.41s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0145, Train Recon Loss: 0.0069, Train KLD Loss: 7.6271\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4300/10000, Epoch time: 0.22s, Total time: 924.46s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0144, Train Recon Loss: 0.0068, Train KLD Loss: 7.6452\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4400/10000, Epoch time: 0.28s, Total time: 949.93s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0145, Train Recon Loss: 0.0068, Train KLD Loss: 7.6542\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4500/10000, Epoch time: 0.28s, Total time: 974.75s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0146, Train Recon Loss: 0.0069, Train KLD Loss: 7.6758\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4600/10000, Epoch time: 0.22s, Total time: 1000.31s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0149, Train Recon Loss: 0.0072, Train KLD Loss: 7.7573\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4700/10000, Epoch time: 0.22s, Total time: 1023.20s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0153, Train Recon Loss: 0.0076, Train KLD Loss: 7.7798\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4800/10000, Epoch time: 0.21s, Total time: 1045.68s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0150, Train Recon Loss: 0.0072, Train KLD Loss: 7.7401\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4900/10000, Epoch time: 0.23s, Total time: 1068.53s, LR: 5.00e-04, KLD Weight: 1.00e-03\n",
      "Train Loss: 0.0145, Train Recon Loss: 0.0068, Train KLD Loss: 7.6958\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(vae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[1;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_kld_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_kld_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, scheduler, n_epochs, plot_every, init_kld_weight, kld_update_every, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m criterion \u001b[38;5;241m=\u001b[39m WeightedLoss(kld_weight)\n\u001b[0;32m---> 27\u001b[0m train_loss, train_recon_loss, train_kld_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m plot_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m running_loss, running_recon_loss, running_kld_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size_batch, interval_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     size_batch, interval_batch \u001b[38;5;241m=\u001b[39m size_batch\u001b[38;5;241m.\u001b[39mto(device), interval_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([size_batch, interval_batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/helen/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/helen/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/helen/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/helen/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/helen/lib/python3.8/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/helen/lib/python3.8/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "plot_every = 100\n",
    "update_kld_every = 1000\n",
    "init_kld_weight = 1e-3\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.8)\n",
    "train(vae, dataloader, optimizer, scheduler, n_epochs, plot_every, init_kld_weight, update_kld_every, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_gen, interval_gen = vae.generate(len(size_probs))\n",
    "size_gen, interval_gen = size_gen.detach().cpu().numpy(), interval_gen.detach().cpu().numpy()\n",
    "size_gen[size_gen < 1e-4] = 0\n",
    "size_gen = size_gen / size_gen.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(ori, recon):\n",
    "    return np.sqrt(np.sum((ori[:, np.newaxis, :] - recon[np.newaxis, :, :]) ** 2, axis=2))\n",
    "    # mae = np.sum(np.abs(ori[:, np.newaxis, :] - recon[np.newaxis, :, :]), axis=2)\n",
    "    # return mse, mae\n",
    "mse = get_mse(size_probs, size_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, mean: 0.017, p90: 0.051, p95: 0.090, p99: 0.203\n",
      "Coverage, mean: 0.018, p90: 0.050, p95: 0.073, p99: 0.164\n"
     ]
    }
   ],
   "source": [
    "mse_accuracy = mse.min(axis=0)\n",
    "mse_coverage = mse.min(axis=1)\n",
    "print(f'Accuracy, mean: {mse_accuracy.mean():.3f}, p90: {np.percentile(mse_accuracy, 90):.3f}, p95: {np.percentile(mse_accuracy, 95):.3f}, p99: {np.percentile(mse_accuracy, 99):.3f}')\n",
    "print(f'Coverage, mean: {mse_coverage.mean():.3f}, p90: {np.percentile(mse_coverage, 90):.3f}, p95: {np.percentile(mse_coverage, 95):.3f}, p99: {np.percentile(mse_coverage, 99):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(ori, recon):\n",
    "    return np.sqrt(np.sum((ori[:, np.newaxis, :] - recon[np.newaxis, :, :]) ** 2, axis=2))\n",
    "\n",
    "mse_accuracy = mse.min(axis=0)\n",
    "mse_coverage = mse.min(axis=1)\n",
    "print(f'Accuracy, mean: {mse_accuracy.mean():.3f}, p90: {np.percentile(mse_accuracy, 90):.3f}, p95: {np.percentile(mse_accuracy, 95):.3f}, p99: {np.percentile(mse_accuracy, 99):.3f}')\n",
    "print(f'Coverage, mean: {mse_coverage.mean():.3f}, p90: {np.percentile(mse_coverage, 90):.3f}, p95: {np.percentile(mse_coverage, 95):.3f}, p99: {np.percentile(mse_coverage, 99):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.8303059785570975e-05,\n",
       " 0.00018246617818991768,\n",
       " 0.00040697942087060683,\n",
       " 0.0009566476372602447,\n",
       " 0.004010271883551145,\n",
       " 0.011588841660199946,\n",
       " 0.023936840160009524,\n",
       " 0.05061360908652382]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.percentile(mse_accuracy, i) for i in range(10, 100, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_size_prob = size_probs.mean(axis=0)\n",
    "avg_diff = np.sum((size_probs - avg_size_prob) ** 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03520654593132698,\n",
       " 0.6344232657271119,\n",
       " 0.9394095746923774,\n",
       " 0.9904461265397568,\n",
       " 1.0246181633645304)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(avg_diff), np.mean(avg_diff), np.percentile(avg_diff, 90), np.percentile(avg_diff, 95), np.percentile(avg_diff, 99)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
